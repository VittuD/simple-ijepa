from torchvision import transforms
import torch
import math, warnings


def training_transforms(img_size):
    return transforms.Compose([
        transforms.RandomResizedCrop(size=img_size, scale=[0.3, 1]),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        # transforms.Normalize((0.4467, 0.4398, 0.4066),(0.2603, 0.2566, 0.2713))
    ])


def inference_transforms(img_size):
    return transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        # transforms.Normalize((0.4467, 0.4398, 0.4066),(0.2603, 0.2566, 0.2713))
    ])


def _trunc_normal_(tensor, mean, std, a, b):

    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fill tensor with values from a truncated normal distribution N(mean, std^2), redrawing until within [a, b].

    NOTE: Bounds [a, b] apply after mean/std. Adjust a and b accordingly.

    Args:
        tensor: input torch.Tensor
        mean: normal mean
        std: normal std
        a: lower bound
        b: upper bound

    Example:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)


# -----------------------------
# Optimization / monitoring utils
# -----------------------------

def compute_global_grad_norm(model) -> float:
    """
    Compute the global L2 norm of gradients over all parameters of `model`.

    Returns:
        float: sqrt(sum_i ||grad_i||^2) over all parameters with non-None grad.
               Returns 0.0 if no gradients are present.
    """
    total_sq = 0.0
    for p in model.parameters():
        if p.grad is None:
            continue
        grad = p.grad.detach()
        param_norm = grad.norm(2)
        total_sq += float(param_norm) ** 2

    if total_sq == 0.0:
        return 0.0
    return float(math.sqrt(total_sq))


def compute_global_param_norm(model) -> float:
    """
    Compute the global L2 norm of parameters of `model`.

    Returns:
        float: sqrt(sum_i ||param_i||^2) over all parameters.
               Returns 0.0 if there are no parameters.
    """
    total_sq = 0.0
    for p in model.parameters():
        data = p.detach()
        param_norm = data.norm(2)
        total_sq += float(param_norm) ** 2

    if total_sq == 0.0:
        return 0.0
    return float(math.sqrt(total_sq))


# -----------------------------
# Gating statistics helpers
# -----------------------------
def summarize_open_fraction(open_frac_per_sample: torch.Tensor) -> dict:
    """
    Given a tensor of shape (B,) with per-sample expected open fractions,
    return simple summary stats as Python floats.
    """
    if not isinstance(open_frac_per_sample, torch.Tensor):
        open_frac_per_sample = torch.as_tensor(open_frac_per_sample)

    if open_frac_per_sample.numel() == 0:
        return {"mean": 0.0, "std": 0.0, "min": 0.0, "max": 0.0}

    # Use unbiased=False so std is well-defined even for B == 1
    mean = open_frac_per_sample.mean().item()
    std = open_frac_per_sample.std(unbiased=False).item()
    min_val = open_frac_per_sample.min().item()
    max_val = open_frac_per_sample.max().item()

    return {
        "mean": mean,
        "std": std,
        "min": min_val,
        "max": max_val,
    }


def summarize_gate_values(gate_values_full: torch.Tensor) -> dict:
    """
    Given a tensor of gate values of shape (B, N) in [0, 1],
    compute summary stats over all gates in the batch.
    """
    if not isinstance(gate_values_full, torch.Tensor):
        gate_values_full = torch.as_tensor(gate_values_full)

    if gate_values_full.numel() == 0:
        return {"mean": 0.0, "std": 0.0}

    flat = gate_values_full.view(-1)

    mean = flat.mean().item()
    std = flat.std(unbiased=False).item()

    return {
        "mean": mean,
        "std": std,
    }
