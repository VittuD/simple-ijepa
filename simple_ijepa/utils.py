from torchvision import transforms
from torchvision.utils import save_image
import torch.nn.functional as F
import torch
import math, warnings, os
import numpy as np
from PIL import Image
from typing import Callable, Optional


def training_transforms(img_size):
    return transforms.Compose([
        transforms.RandomResizedCrop(size=img_size, scale=[0.3, 1]),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ToTensor(),
        # transforms.Normalize((0.4467, 0.4398, 0.4066),(0.2603, 0.2566, 0.2713))
    ])


def inference_transforms(img_size):
    return transforms.Compose([
        transforms.Resize(img_size),
        transforms.ToTensor(),
        # transforms.Normalize((0.4467, 0.4398, 0.4066),(0.2603, 0.2566, 0.2713))
    ])


def _trunc_normal_(tensor, mean, std, a, b):

    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)


# cosine EMA schedule (increase from tau_base to one) as defined in https://arxiv.org/abs/2010.07922
# k -> current training step, K -> maximum number of training steps
def update_gamma(k, K, tau_base):
    k = torch.tensor(k, dtype=torch.float32)
    K = torch.tensor(K, dtype=torch.float32)

    tau = 1 - (1 - tau_base) * (torch.cos(torch.pi * k / K) + 1) / 2
    return tau.item()


# -----------------------------
# Gated-specific utilities
# -----------------------------
def save_debug_masks(
    images: torch.Tensor,
    gate_values_full: torch.Tensor,
    epoch: int,
    global_step: int,
    save_root: str,
    image_size: int,
    patch_size: int,
    max_images: int = 8,
) -> str:
    """
    Save a single debug image that contains, for the first few images
    in the batch:

      - top row: original images
      - bottom row: patch-masked images (based on gate_values_full)

    Returns:
        combined_path: path to the saved PNG.
    """
    os.makedirs(os.path.join(save_root, "debug_masks"), exist_ok=True)
    debug_dir = os.path.join(save_root, "debug_masks")

    B, C, H, W = images.shape
    assert H == image_size and W == image_size, "Unexpected image size."

    num_to_save = min(max_images, B)
    H_p = image_size // patch_size
    W_p = image_size // patch_size
    N = H_p * W_p

    assert gate_values_full.shape[0] == B
    assert gate_values_full.shape[1] == N, "gate_values_full has wrong length."

    images_cpu = images[:num_to_save].detach().cpu()
    gates_cpu = gate_values_full[:num_to_save].detach().cpu()  # (K, N)

    masked_images = []

    for idx in range(num_to_save):
        img = images_cpu[idx].clone()
        g = gates_cpu[idx]

        gate_map = g.view(H_p, W_p)
        img_masked = img.clone()

        for ph in range(H_p):
            for pw in range(W_p):
                if gate_map[ph, pw] < 0.5:
                    h0 = ph * patch_size
                    h1 = h0 + patch_size
                    w0 = pw * patch_size
                    w1 = w0 + patch_size

                    img_masked[:, h0:h1, w0:w1] = 0.0

                    size = patch_size
                    diag = torch.arange(size)

                    img_masked[:, h0 + diag, w0 + diag] = 0.7
                    img_masked[:, h0 + diag, w1 - 1 - diag] = 0.7

        masked_images.append(img_masked)

    masked_batch = torch.stack(masked_images, dim=0)  # (K, C, H, W)

    # Build a single grid image:
    #   top row: original
    #   bottom row: masked
    combined_batch = torch.cat([images_cpu, masked_batch], dim=0)  # (2K, C, H, W)

    step_str = f"e{epoch+1:03d}_s{global_step+1:06d}"
    combined_path = os.path.join(debug_dir, f"{step_str}_orig_masked.png")

    save_image(
        combined_batch,
        combined_path,
        nrow=num_to_save,           # K columns -> 2 rows
        normalize=True,
        value_range=(0.0, 1.0),
    )

    return combined_path


class GatedPredictorEncoder(torch.nn.Module):
    """
    Wraps a GatedIJEPA model to expose a plain encoder interface:
        forward(x) -> (B, N, D) token embeddings

    mode = "gated":
        - If gate_layer_index is None:
            Uses the learned gates on final encoder tokens to mix student
            tokens and mask token, then runs the predictor.
        - If gate_layer_index is not None:
            Reuses the *internal* gating path used during training
            (gating after the chosen transformer block), then runs the
            predictor on the final masked tokens.

    mode = "all_open":
        Uses the raw student tokens (no gating anywhere), then runs the
        predictor. This is a clean ablation: "what if we never gated?".
    """

    def __init__(self, ijepa_model, mode: str = "gated"):
        super().__init__()
        assert mode in ("gated", "all_open")
        self.ijepa = ijepa_model
        self.mode = mode

    @torch.inference_mode
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        m = self.ijepa
        device = x.device

        # "all_open": no gating, no masking
        if self.mode == "all_open":
            student_tokens = m.context_encoder(x)  # plain VisionTransformer
            context_tokens = student_tokens

        else:  # "gated"
            depth = len(m.context_encoder.transformer.layers)
            gate_layer_index = getattr(m, "gate_layer_index", None)

            use_internal_gating = (
                gate_layer_index is not None
                and 0 <= gate_layer_index < depth
            )

            if use_internal_gating:
                # Use the same internal gating path as in training
                student_tokens, gate_values, gate_probs, _ = (
                    m._encode_student_with_internal_gating(x)
                )
                context_tokens = student_tokens
            else:
                # Original behavior: gate final encoder tokens outside encoder
                student_tokens = m.context_encoder(x)  # (B, N, D)
                B, N, D = student_tokens.shape

                log_alpha = m.gate_mlp(student_tokens).squeeze(-1)  # (B, N)
                log_alpha = torch.clamp(log_alpha, min=-10.0, max=10.0)
                gate_values, _ = m.gate(log_alpha, training=False)  # (B, N)
                r = gate_values.unsqueeze(-1)

                mask_token = m.mask_token.to(device)
                mask_tokens = mask_token.expand(B, N, D)
                one_minus_r = 1.0 - r

                context_tokens = r * student_tokens + one_minus_r * mask_tokens

        out = m.predictor(
            context_tokens,
            patchify=False,
            pos_embed=False,
        )
        out = F.layer_norm(out, (out.size(-1),))

        return out


# ----------------------------------------------------------------------
# Token similarity / distance matrices & heatmap utilities
# ----------------------------------------------------------------------

def dot_sim_matrix(tokens: torch.Tensor) -> torch.Tensor:
    """
    Simple dot-product similarity between tokens.

    Args:
        tokens: (N, D) tensor.

    Returns:
        (N, N) matrix with entry (i, j) = <x_i, x_j>.
    """
    return tokens @ tokens.T  # (N, N)


def euclidean_dist2_matrix(tokens: torch.Tensor) -> torch.Tensor:
    """
    Squared Euclidean distance matrix.

    Args:
        tokens: (N, D) tensor.

    Returns:
        (N, N) matrix with ||x_i - x_j||^2.
    """
    x = tokens
    # (N,)
    sq_norm = (x ** 2).sum(dim=1)
    # (N, N): ||x_i||^2 + ||x_j||^2 - 2 x_iÂ·x_j
    dist2 = sq_norm.unsqueeze(1) + sq_norm.unsqueeze(0) - 2 * (x @ x.T)
    # numerical safety
    return torch.clamp(dist2, min=0.0)


def rbf_kernel_matrix(tokens: torch.Tensor, sigma: float = 1.0) -> torch.Tensor:
    """
    RBF / Gaussian kernel similarity based on squared Euclidean distance.

    Args:
        tokens: (N, D) tensor.
        sigma:  kernel width.

    Returns:
        (N, N) matrix with exp(-||x_i - x_j||^2 / (2 * sigma^2)).
    """
    dist2 = euclidean_dist2_matrix(tokens)
    return torch.exp(-dist2 / (2 * sigma ** 2))


def token_corr_matrix(tokens: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
    """
    Pearson correlation matrix over token features.

    Args:
        tokens: (N, D) tensor.
        eps:    small constant for numerical stability.

    Returns:
        (N, N) correlation matrix in [-1, 1].
    """
    x = tokens
    N, D = x.shape

    mu = x.mean(dim=1, keepdim=True)        # (N, 1)
    x_centered = x - mu                     # (N, D)

    # Use max(D-1, 1) to avoid division by zero when D == 1
    denom_d = max(D - 1, 1)
    cov = (x_centered @ x_centered.T) / denom_d   # (N, N)
    var = cov.diag()                               # (N,)

    std = torch.sqrt(var + eps)                   # (N,)
    denom = std.unsqueeze(0) * std.unsqueeze(1)   # (N, N)

    corr = cov / (denom + eps)
    return torch.clamp(corr, -1.0, 1.0)


def cosine_sim_matrix(tokens: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
    """
    Cosine similarity matrix between tokens.

    Args:
        tokens: (N, D) tensor.

    Returns:
        (N, N) cosine similarity matrix in [-1, 1].
    """
    x = F.normalize(tokens, p=2, dim=-1, eps=eps)  # (N, D), rows unit norm
    return x @ x.T                                  # (N, N)


def compute_token_ssim_matrix(
    tokens: torch.Tensor,
    c1: float = 1e-4,
    c2: float = 9e-4,
) -> torch.Tensor:
    """
    Compute an "SSIM-style" similarity matrix between token vectors.

    Args:
        tokens: Tensor of shape (N, D), where N is number of tokens and
                D is embedding dimension (e.g., input to gate MLP).
        c1, c2: Small constants for numerical stability, analogous to
                standard SSIM.

    Returns:
        ssim: Tensor of shape (N, N), where ssim[i, j] is the structural
              similarity between token i and token j (in embedding space).
    """
    # tokens: (N, D)
    x = tokens  # (N, D)
    N, D = x.shape

    # Mean per token (over feature dim)
    mu = x.mean(dim=1, keepdim=True)          # (N, 1)
    x_centered = x - mu                       # (N, D)
    var = (x_centered ** 2).mean(dim=1, keepdim=True)  # (N, 1)

    # Covariance between token i and j:
    # cov[i,j] = E_d[(x_i_d - mu_i)*(x_j_d - mu_j)]
    cov = (x_centered @ x_centered.T) / D     # (N, N)

    mu_i = mu                                 # (N, 1)
    mu_j = mu.T                               # (1, N)
    var_i = var                               # (N, 1)
    var_j = var.T                             # (1, N)

    num = (2 * mu_i * mu_j + c1) * (2 * cov + c2)
    den = (mu_i ** 2 + mu_j ** 2 + c1) * (var_i + var_j + c2)

    ssim = num / (den + 1e-12)
    ssim = torch.clamp(ssim, -1.0, 1.0)
    return ssim


def average_pairwise_cosine(
    tokens: torch.Tensor,
    eps: float = 1e-12,
) -> torch.Tensor:
    """
    Average pairwise cosine similarity between tokens for a single example.

    Args:
        tokens: (N, D) tensor of token embeddings for ONE sample.
        eps:    numerical stability constant.

    Returns:
        scalar tensor: mean_{i != j} cos(x_i, x_j).
    """
    # tokens: (N, D)
    if tokens.ndim != 2:
        raise ValueError(f"expected tokens to have shape (N, D), got {tokens.shape}")

    N, D = tokens.shape
    if N <= 1:
        # No pairs; return 0 as a neutral value
        return torch.zeros((), device=tokens.device, dtype=tokens.dtype)

    # Normalize per token
    x = F.normalize(tokens, p=2, dim=-1, eps=eps)  # (N, D)
    sim = x @ x.T                                  # (N, N), cosine similarities

    # Exclude diagonal (self-similarity)
    mask = ~torch.eye(N, dtype=torch.bool, device=sim.device)
    return sim[mask].mean()


def average_pairwise_cosine_batch(
    tokens_batch: torch.Tensor,
    eps: float = 1e-12,
) -> torch.Tensor:
    """
    Batch version of average_pairwise_cosine.

    Args:
        tokens_batch: (K, N, D) tensor; K examples, N tokens, D dim.
        eps:          numerical stability constant.

    Returns:
        scalar tensor: average over K samples of their per-sample ACS.
    """
    if tokens_batch.ndim != 3:
        raise ValueError(f"expected tokens_batch to have shape (K, N, D), got {tokens_batch.shape}")

    K, N, D = tokens_batch.shape
    if K == 0:
        return torch.zeros((), device=tokens_batch.device, dtype=tokens_batch.dtype)

    vals = []
    for k in range(K):
        vals.append(average_pairwise_cosine(tokens_batch[k], eps=eps))

    return torch.stack(vals).mean()


def effective_rank(
    tokens: torch.Tensor,
    eps: float = 1e-12,
) -> torch.Tensor:
    """
    Effective rank (erank) of the token matrix for a single example.

    Uses Shannon entropy of the squared singular values:

        erank = exp( H(p) ),  p_i = s_i^2 / sum_j s_j^2

    where s_i are singular values of the centered token matrix.

    Args:
        tokens: (N, D) tensor of token embeddings for ONE sample.
        eps:    numerical stability constant.

    Returns:
        scalar tensor: effective rank (>= 1).
    """
    if tokens.ndim != 2:
        raise ValueError(f"expected tokens to have shape (N, D), got {tokens.shape}")

    # Center across tokens (treat tokens as samples, features along dim=1)
    x = tokens - tokens.mean(dim=0, keepdim=True)  # (N, D)

    # SVD: x = U S V^T, we only need singular values S
    # full_matrices=False for efficiency
    _, S, _ = torch.linalg.svd(x, full_matrices=False)  # S: (min(N, D),)

    # Convert singular values to a probability distribution over energy
    energy = S ** 2
    energy_sum = energy.sum()
    energy = energy / (energy_sum + eps)

    # Shannon entropy of spectrum (base e)
    H = -(energy * (energy + eps).log()).sum()

    erank = torch.exp(H)
    return erank


def effective_rank_batch(
    tokens_batch: torch.Tensor,
    eps: float = 1e-12,
) -> torch.Tensor:
    """
    Batch version of effective_rank.

    Args:
        tokens_batch: (K, N, D) tensor of token embeddings.
        eps:          numerical stability constant.

    Returns:
        scalar tensor: average effective rank across the K samples.
    """
    if tokens_batch.ndim != 3:
        raise ValueError(f"expected tokens_batch to have shape (K, N, D), got {tokens_batch.shape}")

    K, N, D = tokens_batch.shape
    if K == 0:
        return torch.zeros((), device=tokens_batch.device, dtype=tokens_batch.dtype)

    vals = []
    for k in range(K):
        vals.append(effective_rank(tokens_batch[k], eps=eps))

    return torch.stack(vals).mean()


def _normalize_to_uint8(
    mat: np.ndarray,
    value_range: Optional[tuple[float, float]] = None,
) -> np.ndarray:
    """
    Normalize a float matrix to uint8 [0, 255] for visualization.
    """
    mat = np.asarray(mat, dtype=np.float32)

    if mat.size == 0:
        return np.zeros_like(mat, dtype=np.uint8)

    if value_range is not None:
        vmin, vmax = value_range
    else:
        vmin = float(np.nanmin(mat))
        vmax = float(np.nanmax(mat))

    # Replace non-finite entries with the minimum
    mat = np.where(np.isfinite(mat), mat, vmin)

    if vmax <= vmin:
        return np.zeros_like(mat, dtype=np.uint8)

    mat = (mat - vmin) / (vmax - vmin)
    mat = np.clip(mat, 0.0, 1.0)
    return (mat * 255.0).astype(np.uint8)


def save_heatmap_from_matrix(
    matrix: torch.Tensor | np.ndarray,
    out_path: str,
    scale: int = 4,
    value_range: Optional[tuple[float, float]] = None,
) -> None:
    """
    Save a precomputed similarity/distance matrix as a grayscale heatmap,
    where each cell is rendered as a `scale` x `scale` block.

    Args:
        matrix:      (N, N) similarity / distance matrix.
        out_path:    Path to the PNG file.
        scale:       Integer upscale factor for each cell.
        value_range: Optional (vmin, vmax) for normalization. If None,
                     uses matrix min/max.
    """
    if isinstance(matrix, torch.Tensor):
        mat_np = matrix.detach().cpu().numpy().astype(np.float32)
    else:
        mat_np = np.asarray(matrix, dtype=np.float32)

    img_arr = _normalize_to_uint8(mat_np, value_range=value_range)

    if scale > 1:
        img_arr = np.repeat(np.repeat(img_arr, scale, axis=0), scale, axis=1)

    img = Image.fromarray(img_arr, mode="L")
    img.save(out_path)


def save_heatmap(
    tokens: torch.Tensor,
    out_path: str,
    metric_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
    scale: int = 4,
    value_range: Optional[tuple[float, float]] = None,
) -> None:
    """
    Generic heatmap saver: compute a token-token metric matrix with
    `metric_fn(tokens)` and save it as a grayscale image.

    By default, uses cosine similarity.

    Args:
        tokens:     (N, D) token embeddings.
        out_path:   Path to the PNG file.
        metric_fn:  Callable mapping (N, D) -> (N, N). Default is
                    cosine_sim_matrix.
        scale:      Integer upscale factor for each cell.
        value_range:Optional (vmin, vmax) for normalization. If None,
                    uses matrix min/max.
    """
    if metric_fn is None:
        metric_fn = cosine_sim_matrix

    matrix = metric_fn(tokens)
    save_heatmap_from_matrix(
        matrix,
        out_path=out_path,
        scale=scale,
        value_range=value_range,
    )


def save_sim_heatmap_grid(
    tokens_batch: torch.Tensor,
    out_path: str,
    metric_fn: Optional[Callable[[torch.Tensor], torch.Tensor]] = None,
    scale: int = 4,
    pad_width: int = 4,
    pad_value: float = 0.2,
    max_cols: int = 8,
    value_range: Optional[tuple[float, float]] = None,
) -> None:
    """
    Save a PNG containing a grid of heatmaps (one per example), using a
    token-token metric.

    Layout:
      - up to `max_cols` heatmaps per row
      - each heatmap has vertical separator bars on left and right

    Args:
        tokens_batch: (K, N, D) tensor; K examples, N tokens, D dim.
        out_path:     Where to save the combined PNG.
        metric_fn:    Callable mapping (N, D) -> (N, N).
                      Defaults to cosine similarity (cosine_sim_matrix).
        scale:        Upscaling factor per cell (visual size).
        pad_width:    Pixel width of vertical separator bars.
        pad_value:    Grayscale value in [0, 1] for separator bars.
        max_cols:     Maximum number of heatmaps per row.
        value_range:  Optional (vmin, vmax) for normalization across ALL
                      tiles. If None, uses global min/max over all matrices.
    """
    tokens_batch = tokens_batch.detach().cpu()
    K, N, D = tokens_batch.shape

    if K == 0:
        return  # nothing to save

    if metric_fn is None:
        # Default: cosine similarity
        metric_fn = cosine_sim_matrix

    # ---------------------------------------------------------
    # 1) Compute all metric matrices and gather global min/max
    # ---------------------------------------------------------
    matrices: list[np.ndarray] = []
    global_min = float("inf")
    global_max = float("-inf")

    for k in range(K):
        tokens = tokens_batch[k]  # (N, D)
        mat = metric_fn(tokens)   # (N, N)

        if isinstance(mat, torch.Tensor):
            mat_np = mat.detach().cpu().numpy().astype(np.float32)
        else:
            mat_np = np.asarray(mat, dtype=np.float32)

        matrices.append(mat_np)

        if value_range is None:
            finite = mat_np[np.isfinite(mat_np)]
            if finite.size > 0:
                mmin = float(finite.min())
                mmax = float(finite.max())
                global_min = min(global_min, mmin)
                global_max = max(global_max, mmax)

    # Decide normalization range
    if value_range is not None:
        vmin, vmax = value_range
    else:
        if not np.isfinite(global_min) or not np.isfinite(global_max) or global_max <= global_min:
            vmin, vmax = 0.0, 1.0
        else:
            vmin, vmax = global_min, global_max

    pad_val_255 = int(np.clip(pad_value, 0.0, 1.0) * 255)

    # ---------------------------------------------------------
    # 2) Build per-example tiles with vertical separator bars
    # ---------------------------------------------------------
    tiles: list[np.ndarray] = []
    for mat_np in matrices:
        img_arr = _normalize_to_uint8(mat_np, value_range=(vmin, vmax))

        if scale > 1:
            img_arr = np.repeat(np.repeat(img_arr, scale, axis=0), scale, axis=1)

        H, W = img_arr.shape

        # Vertical separator bars on left and right
        pad_col = np.full((H, pad_width), pad_val_255, dtype=np.uint8)
        tile_with_bars = np.concatenate(
            [pad_col, img_arr, pad_col],
            axis=1,
        )  # (H, W + 2 * pad_width)

        tiles.append(tile_with_bars)

    # ---------------------------------------------------------
    # 3) Arrange tiles into a grid (rows x cols)
    # ---------------------------------------------------------
    num_cols = min(max_cols, K)
    num_rows = int(math.ceil(K / num_cols))

    row_images: list[np.ndarray] = []
    for r in range(num_rows):
        row_tiles = tiles[r * num_cols : (r + 1) * num_cols]
        if not row_tiles:
            continue
        row_img = np.concatenate(row_tiles, axis=1)
        row_images.append(row_img)

    combined = np.concatenate(row_images, axis=0)  # stack rows vertically

    img = Image.fromarray(combined, mode="L")
    img.save(out_path)
